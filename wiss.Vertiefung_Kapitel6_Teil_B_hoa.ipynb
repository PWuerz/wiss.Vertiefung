{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "#### Project: https://github.com/vwxyzjn/gym-microrts/blob/master/experiments/paper/ppo_diverse.py\nClass: **ppo_diverse.py**  -> pytorch \nCodezeile: 262-352 -> **Agent** initialize ",
   "metadata": {
    "tags": [],
    "cell_id": "00001-754b8ff7-2562-49a7-8181-a07a25c85a88",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-9099cbb1-13d4-40f0-b15a-bf2d33bc40e4",
    "deepnote_cell_type": "code"
   },
   "source": "args.batch_size = int(args.num_envs * args.num_steps)\nargs.minibatch_size = int(args.batch_size // args.n_minibatch)\n\n...\n\n# flatten the batch\nb_obs = obs.reshape((-1,) + envs.observation_space.shape)\nb_logprobs = logprobs.reshape(-1)\nb_actions = actions.reshape((-1,) + envs.action_space.shape)\nb_advantages = advantages.reshape(-1)\nb_returns = returns.reshape(-1)\nb_values = values.reshape(-1)\nb_invalid_action_masks = invalid_action_masks.reshape((-1, invalid_action_masks.shape[-1]))\n\n# Optimizaing the policy and value network\ntarget_agent = Agent().to(device)\ninds = np.arange(\n    args.batch_size,\n)\nfor i_epoch_pi in range(args.update_epochs):\n    np.random.shuffle(inds)\n    for start in range(0, args.batch_size, args.minibatch_size):\n        end = start + args.minibatch_size\n        minibatch_ind = inds[start:end]\n        mb_advantages = b_advantages[minibatch_ind]\n        if args.norm_adv:\n            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n        _, newlogproba, entropy, _, new_values = agent.get_action_and_value(\n            b_obs[minibatch_ind], b_actions.long()[minibatch_ind], b_invalid_action_masks[minibatch_ind], envs, device\n        )\n        ratio = (newlogproba - b_logprobs[minibatch_ind]).exp()\n\n        # Stats\n        approx_kl = (b_logprobs[minibatch_ind] - newlogproba).mean()\n\n        # Policy loss\n        pg_loss1 = -mb_advantages * ratio\n        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n        entropy_loss = entropy.mean()\n\n        # Value loss\n        new_values = new_values.view(-1)\n        if args.clip_vloss:\n            v_loss_unclipped = (new_values - b_returns[minibatch_ind]) ** 2\n            v_clipped = b_values[minibatch_ind] + torch.clamp(\n                new_values - b_values[minibatch_ind], -args.clip_coef, args.clip_coef\n            )\n            v_loss_clipped = (v_clipped - b_returns[minibatch_ind]) ** 2\n            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n            v_loss = 0.5 * v_loss_max.mean()\n        else:\n            v_loss = 0.5 * ((new_values - b_returns[minibatch_ind]) ** 2)\n\n        loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n        optimizer.step()\n\n    if args.kle_stop:\n         if approx_kl > args.target_kl:\n              break\n    if args.kle_rollback:\n        if (\n            b_logprobs[minibatch_ind]\n            - agent.get_action(\n                b_obs[minibatch_ind], b_actions.long()[minibatch_ind].T, b_invalid_action_masks[minibatch_ind], envs\n             )[1]\n        ).mean() > args.target_kl:\n            agent.load_state_dict(target_agent.state_dict())\n            break\n\n...\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Der folgende Codeabschnitt aus ppo_diverse.py beschreibt die Verwendung von PPO, wodurch der Agent eine Optimierung der Policy erh√§lt. In folgenden wird das Training f√ºr K Epochen in der Codezeile 20 \"for i_epoch_pi in range(args.update_epochs)\" durchgef√ºhrt. Dabei werden die Daten und die daraus ausgew√§hlte mini-batches(minibatch_ind) f√ºr das Training gesammelt und genutzt. Das Wahrscheinlichkeitsverh√§ltnis **\"ratio\"** wird in der Codezeile 31 zwischen der alten Policy und der neuen Policy definiert. \n\nAb der Zeile 36-40 wird die \"Policy Loss\"-Funktion definiert. Hierbei beschreibt **\"pg_loss1\"** den \"unclipped objective\"-Term mit der Berechnung des Wahrscheinlichkeitsverh√§ltnis und des Advantages. **\"pg_loss2\"** beschreibt den \"clipped Objective\"-Term, in der das Wahrscheinlichkeitsverh√§ltnis in einem Bereich  zwischen **1-ùúñ** und **1+ùúñ** abgeschnitten wird und auch mit der Advantages berechnet wird. Die ‚ÄûClipped Surrogate Objective function‚Äú wird in der Codezeile 39 dargestellt, welches das Wahrscheinlichkeitsverh√§ltnis unter der neuen bzw. alten Policy zwischen einem bestimmten Bereich abschneidet und nicht zul√§sst, dass sie sich weiter von diesem Bereich entfernt.\n\nIm n√§chsten Schritt wird die **\"Value Loss\"-Funktion** bzw. ‚ÄúClipped Surrogate Objective Loss‚Äú-Funktion ab der Codezeile 43-53 berechnet, die hierbei den ‚Äûsquared-error value loss‚Äú bzw. ein mittlerer quadratischer Fehler der Wertfunktion beschreibt. Hier wird versucht die Differenz zwischen dem gesch√§tzten Wert und dem tats√§chlichen Wert zu minimieren.\n\nDie endg√ºltige ‚ÄúClipped Surrogate Objective Loss‚Äú-Funktion wird in der Codezeile 55 **\"loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\"** definiert. Die Variablen **\"args.ent_coef\"** und **\"args.vf_coef\"** beschreiben die Hyperparameter. Die Variable **\"v_loss\"** und **\"entropy_loss\"** sind die zus√§tzlichen Terme, die bei der endg√ºltige ‚ÄúClipped Surrogate Objective Loss‚Äú-Funktion erg√§nzt werden. **\"v_loss\"** definiert den Fehlerterm f√ºr die Wertsch√§tzung und **\"entropy_loss\"** beschreibt einen Entropieterm, der f√ºr die ausreichende Exploration des Agenten f√∂rdert.\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00002-e2691d60-e21a-4e4c-a4e1-6368f05fca32",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Project: https://github.com/vwxyzjn/gym-microrts/blob/master/experiments/ppo_gridnet.py\n\nClass: ppo_gridnet.py",
   "metadata": {
    "tags": [],
    "cell_id": "00003-6d729365-44ef-4d66-8ba4-1f3862fbe8cc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-e28db951-973d-429f-9bd7-41777ff71276",
    "deepnote_cell_type": "code"
   },
   "source": "args.batch_size = int(args.num_envs * args.num_steps)\nargs.minibatch_size = int(args.batch_size // args.n_minibatch)\n\n...\n\n# flatten the batch\nb_obs = obs.reshape((-1,) + envs.observation_space.shape)\nb_logprobs = logprobs.reshape(-1)\nb_actions = actions.reshape((-1,) + action_space_shape)\nb_advantages = advantages.reshape(-1)\nb_returns = returns.reshape(-1)\nb_values = values.reshape(-1)\nb_invalid_action_masks = invalid_action_masks.reshape((-1,) + invalid_action_shape)\n\n# Optimizaing the policy and value network\ninds = np.arange(\n    args.batch_size,\n)\nfor i_epoch_pi in range(args.update_epochs):\n    np.random.shuffle(inds)\n    for start in range(0, args.batch_size, args.minibatch_size):\n        end = start + args.minibatch_size\n        minibatch_ind = inds[start:end]\n        mb_advantages = b_advantages[minibatch_ind]\n        if args.norm_adv:\n            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n        _, newlogproba, entropy, _, new_values = agent.get_action_and_value(\n            b_obs[minibatch_ind], b_actions.long()[minibatch_ind], b_invalid_action_masks[minibatch_ind], envs, device\n        )\n        ratio = (newlogproba - b_logprobs[minibatch_ind]).exp()\n\n        # Stats\n        approx_kl = (b_logprobs[minibatch_ind] - newlogproba).mean()\n\n        # Policy loss\n        pg_loss1 = -mb_advantages * ratio\n        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n        entropy_loss = entropy.mean()\n\n        # Value loss\n        new_values = new_values.view(-1)\n        if args.clip_vloss:\n            v_loss_unclipped = (new_values - b_returns[minibatch_ind]) ** 2\n            v_clipped = b_values[minibatch_ind] + torch.clamp(\n                new_values - b_values[minibatch_ind], -args.clip_coef, args.clip_coef\n            )\n            v_loss_clipped = (v_clipped - b_returns[minibatch_ind]) ** 2\n            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n            v_loss = 0.5 * v_loss_max.mean()\n        else:\n            v_loss = 0.5 * ((new_values - b_returns[minibatch_ind]) ** 2)\n\n        loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n        optimizer.step()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Project: https://github.com/openai/baselines/blob/9fa8e1baf1d1f975b87b369a8082122eac812eb1/baselines/ppo1/pposgd_simple.py#L111-L117\n\nclass: ppo1/pposgd_simple.py   mit Tensorflow\nBaseline Implementierung von PPO von OpenAI",
   "metadata": {
    "tags": [],
    "cell_id": "00002-4f671e10-0264-498c-be40-4a5f825761fa",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-b03af479-6456-4dec-b024-90d3d75281a5",
    "deepnote_cell_type": "code"
   },
   "source": "    ratio = tf.exp(pi.pd.logp(ac) - oldpi.pd.logp(ac)) # pnew / pold\n    surr1 = ratio * atarg # surrogate from conservative policy iteration\n    surr2 = tf.clip_by_value(ratio, 1.0 - clip_param, 1.0 + clip_param) * atarg #\n    pol_surr = - tf.reduce_mean(tf.minimum(surr1, surr2)) # PPO's pessimistic surrogate (L^CLIP)\n    vf_loss = tf.reduce_mean(tf.square(pi.vpred - ret))\n    total_loss = pol_surr + pol_entpen + vf_loss\n    losses = [pol_surr, pol_entpen, vf_loss, meankl, meanent]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Der folgende Codeabschnitt aus der Baseline von **pposgd_simple.py**: \n\n**ratio** -> Wahrscheinlichkeitsverh√§ltnis zwischen der neuen und alten Policy\n\n**surr1** -> Unclipped Objective \n\n**surr2** -> Clipped Objective, Beschneidung zwischen 1-ùúñ und 1+ùúñ\n\n**pol_surr** -> ‚ÄûClipped Surrogate Objective\"-Funktion (Clipping Mechanismus von PPO)\n\n**vf_loss** -> Beschreibt den ‚Äûsquared-error value loss‚Äú bzw. ein mittlerer quadratischer Fehler der Wertfunktion\n\n**total_loss** -> Endg√ºltige ‚ÄúClipped Surrogate Objective Loss‚Äú-Funktion, welches f√ºr das Trainieren eines Agenten genutzt wird. ",
   "metadata": {
    "tags": [],
    "cell_id": "00005-2bc6a46f-1748-4bd3-842a-bab25c287c33",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Project: https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb\n\nClass:RL-Adventure-2/3.ppo.ipynb -> mit pytorch",
   "metadata": {
    "tags": [],
    "cell_id": "00004-62aa6712-2db2-4e99-ba0c-1286bbf7ba58",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-43570dab-aa7f-4fb5-a31f-edafe3a453c4",
    "deepnote_cell_type": "code"
   },
   "source": "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n    batch_size = states.size(0)\n    for _ in range(batch_size // mini_batch_size):\n        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n        \n        \n\ndef ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n    for _ in range(ppo_epochs):\n        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n            dist, value = model(state)\n            entropy = dist.entropy().mean()\n            new_log_probs = dist.log_prob(action)\n\n            ratio = (new_log_probs - old_log_probs).exp()\n            surr1 = ratio * advantage\n            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n\n            actor_loss  = - torch.min(surr1, surr2).mean()\n            critic_loss = (return_ - value).pow(2).mean()\n\n            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Der folgende Codeabschnitt mit der Funktionen **\"ppo_iter\"** und **\"ppo_update\"** beschreiben die mehrere Epochen f√ºr die Aktualisierung der Policy und das Clipping Mechanismus, welches die Trainingsstabilit√§t verbessern soll, indem es die √Ñnderungen an der Policy bei jedem Schritt begrenzt. \n\nIn der Funktion \"ppo_update\" werden mehrere Epochen des Gradientenanstiegs f√ºr die \"Samples\" durchgef√ºhrt. Hierbei beschreibt **\"ratio\"** in der Codezeile 16 das Wahrscheinlichkeitsverh√§ltnis (alten Policy und der neuen Policy).\n\nDie Funktion **\"actor_loss\"** beschreibt die ‚ÄûClipped Surrogate Objective\"-Funktion, welches aus den Funktionen **\"surr1\"** (unclipped objective) und **\"surr2\"** (clipped objective) besteht.\n\nDie Funktion **\"critic_loss\"** versucht die Differenz zwischen dem gesch√§tzten Wert und dem tats√§chlichen Wert zu minimieren (\"squared-error value loss\"-Funktion)\n\nDie endg√ºltige ‚ÄúClipped Surrogate Objective Loss‚Äú Funktion wird in der Codezeile 23 **\"loss\"** definiert, welches aus den Werten der ‚ÄúClipped Surrogate Objective Loss‚Äú Funktion(actor_loss) mit den zwei zus√§tzlichen Termen ‚Äûsquared-error value loss‚Äú(critic-loss) und Entropieterm (entropy) und deren Hyperparameter berechnet wird.",
   "metadata": {
    "tags": [],
    "cell_id": "00006-7a52ab4c-48a2-47fc-93f9-8cfb3ef46f1c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-9c1094c4-1c4c-4ae1-b9e9-3b802b826d6a",
    "deepnote_cell_type": "code"
   },
   "source": "\nnum_inputs  = envs.observation_space.shape[0]\nnum_outputs = envs.action_space.shape[0]\n\n#Hyper params:\nhidden_size      = 256\nlr               = 3e-4\nnum_steps        = 20\nmini_batch_size  = 5\nppo_epochs       = 4\nthreshold_reward = -200\n\nmodel = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Hier werden die Werte f√ºr die Hyperparameter definiert.",
   "metadata": {
    "tags": [],
    "cell_id": "00007-3f5cfa8e-3f75-429d-b1a8-bd1d20aaae5f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9ce75853-4372-4d76-b47c-34c3ad90a24b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "e0299634-e466-4abe-b7d8-3ecdce7b8b66",
  "deepnote_execution_queue": []
 }
}